import os
import pefile
from time import time, sleep
from hashlib import md5
from binascii import hexlify
import yara
import regex as re
import logging
from multiprocessing import Pool, cpu_count

"""
Most of this code from JHumble https://raw.githubusercontent.com/jhumble/Unpackers-and-Config-Extractors/master/lib/utils.py
"""

def configure_logger(log_level):
    log_levels = {0: logging.ERROR, 1: logging.WARNING, 2: logging.INFO, 3: logging.DEBUG}
    log_level = min(max(log_level, 0), 3) #clamp to 0-3 inclusive
    logging.basicConfig(level=log_levels[log_level],
            format='%(asctime)s - %(name)s - %(levelname)-8s %(message)s')

def get_compile_time(pe):
    ts = int(pe.FILE_HEADER.dump_dict()['TimeDateStamp']['Value'].split()[0], 16)
    utc_time = datetime.datetime.utcfromtimestamp(ts)
    t_delta = (datetime.datetime.today() - utc_time).days
    return utc_time.strftime(f"%Y-%m-%dT%H:%M:%S")

def rol(dword, i):
    return ((dword << i) & 0xffffffff) | (dword >> (32-i) ) 

def ror(dword, i):
    return rol(dword, 32-i)

def xor(plaintext, key):
    rtn = bytearray(len(plaintext))
    for i in range(len(plaintext)):
        rtn[i] = plaintext[i] ^ key[i%len(key)]
    return rtn

def carve(buf, match_at_start=False):
    found = []
    for i in [match.start() for match in re.finditer(b'MZ', buf)]:
        if i == 0 and not match_at_start: # Ignore matches at offset 0 (regular PE files)
            continue
        try:
            pe = pefile.PE(data=buf[i:])
        except:
            continue
        #print(f'Found PE file at offset 0x{i:X}')
        found.append({'offset': i, 'data': pe.trim(), 'ext': get_ext(pe) })
    return found


def get_ext(pe):
    # https://github.com/MalwareLu/tools/blob/master/pe-carv.py
    'returns ext of the file type using pefile'
    if pe.is_dll() == True:
        return 'dll'
    if pe.is_driver() == True:
        return 'sys'
    if pe.is_exe() == True:
        return 'exe'
    else:
        return 'bin'

def iter_resources(pe):
    for rsrc in pe.DIRECTORY_ENTRY_RESOURCE.entries:
        for entry in rsrc.directory.entries:
            if rsrc.name:
                name = f'{rsrc.name}/{entry.name}'
            else:
                name = entry.name
            offset = entry.directory.entries[0].data.struct.OffsetToData
            size = entry.directory.entries[0].data.struct.Size
            #print(f'{size:08X}')
            if entry.directory.entries[0].name:
                _id = entry.directory.entries[0].name
            else:
                _id = '0x{:X}'.format(entry.directory.entries[0].id)
            #print(f'{offset:08X} {size:08X}')
            data = bytearray(pe.get_memory_mapped_image()[offset:offset+size])
            #print(f'Found resource {name}/{_id}, length: {len(data):08X}')
            yield name, _id, data

def scan_dir(target_dir, rules):
    match_list = dict()
    for file in os.listdir(target_dir):
        path = os.path.join(target_dir, file)
        try:
            matches = rules.match(path)
            if len(matches) > 0:
                match_list[path] = matches[0].rule
        except Exception as e:
            pass
    return match_list

def build_rules(signature_dir, profile_rules=False):
    logger = logging.getLogger('Yara Compiler')
    file_list = recursive_all_files(signature_dir,'yar')
    _hash = rules_hash(file_list)
    if profile_rules:
        return test_compile(file_list, individual_rules=True)
    path = os.path.join('/tmp/', '%s.py3.cyar' % (_hash))
    if os.path.isfile(path):
        logger.debug('Up to date compiled rules already exist at %s. Using those' % (path))
        return yara.load(path)

    start = time()
    rulefile_paths = test_compile(file_list)
    elapsed = time() - start
    logger.debug('Test compiled %s rules in %s seconds.' % (len(rulefile_paths), round(elapsed,2)))

    start = time()
    try:
        compiled_rules = yara.compile(filepaths=rulefile_paths)
    except Exception as e:
        logger.error('Exception compiling rules: %s' % (e))
    elapsed = time() - start
    try:
        compiled_rules.save(path)
        os.chmod(path, 0o666)
    except Exception as e:
        logger.debug('Failed to save compiled rules %s: %s' % (path,e))
    compiled_size = os.stat(path).st_size

    logger.debug('Compiled %s rules in %s seconds.' % (len(rulefile_paths), round(elapsed,2)))
    logger.debug('Compiled rule size is %s' % (human_size(compiled_size,)))
    return compiled_rules

def recursive_all_files(directory, ext_filter=None):
    all_files = []
    dir_content = []
    ret = []

    if os.path.isfile(directory):
        dir_content = [directory]
    else:
        try:
            dir_content = os.listdir(directory)
        except Exception as e:
            return []

    for f in dir_content:
        if os.path.isdir(directory):
            rel_path = os.path.join(directory,f)
        else:
            rel_path = f
        if os.path.isfile(rel_path):
            all_files.append(rel_path)
        elif f == '.' or f == '..':
            pass
        else:
            all_files += recursive_all_files(rel_path,ext_filter)

    for f in all_files:
        if (ext_filter is None or os.path.splitext(f)[1] == '.%s' % ext_filter):
            ret.append(f)
    return ret

def rules_hash(file_list):
    rtn = {}
    to_hash = ""
    for path in sorted(file_list):
        to_hash += '%s%s' % (os.path.basename(path),str(os.path.getmtime(path)))
    return md5(to_hash.encode()).hexdigest()

def spaced_hex(data):
    return b' '.join(hexlify(data)[i:i + 2] for i in range(0, len(hexlify(data)), 2)).decode('utf-8')

def test_compile(file_list, individual_rules=False):
    rtn = {}
    # Can't use a pool since we can't pickle the compiled rule objects to send across the queue
    if individual_rules:
        for f in file_list:
            compiled = compile_rule(f, include_compiled=True)
            if compiled:
                rtn[compiled['key']] = compiled
    
    else:
        pool = Pool(4)
        results = pool.map(compile_rule, file_list)
        for item in results:
            if item:
                rtn[item['key']] = item['rulefile']
    return rtn
